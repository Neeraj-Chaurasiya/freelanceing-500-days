# üìÖ Day 13 ‚Äì Web Scraping with Multiple Pages

## ‚úÖ Topics Covered

* **Target Website & Libraries**
* **Loop Through Multiple Pages**
* **Data Extraction (Title, Price, Rating)**
* **Store Data in CSV**
* **Practice Task**

---

## 1Ô∏è‚É£ Target Website & Libraries

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

base_url = "https://books.toscrape.com/catalogue/page-{}.html"
```

* `requests` ‚Üí Send HTTP requests to fetch HTML.
* `BeautifulSoup` ‚Üí Parse HTML and extract data.
* `pandas` ‚Üí Store data in tabular form (DataFrame) and save to CSV.
* `base_url` ‚Üí Website pattern for multiple pages (`{}` ‚Üí placeholder for page number).

---

## 2Ô∏è‚É£ Loop Through Multiple Pages

```python
titles = []
prices = []
ratings  = []

p = int(input("How many pages you hai to extract : "))
```

* Empty lists created for `titles`, `prices`, and `ratings`.
* `input()` ‚Üí Ask user how many pages to scrape.
* `int()` ‚Üí Convert input into integer.

```python
for page in range(p):
  url = base_url.format(page)
  response = requests.get(url)
  soup = BeautifulSoup(response.text ,"html.parser")

  books = soup.find_all("article", class_="product_pod")

  for book in books:
    title = book.h3.a['title']
    price = book.find("p", class_="price_color").text
    rating = book.p["class"][1]
    titles.append(title)
    prices.append(price)
    ratings.append(rating)
```

* `format(page)` ‚Üí Insert page number in URL.
* `requests.get(url)` ‚Üí Fetch that page‚Äôs HTML.
* `soup.find_all()` ‚Üí Select all book containers.
* Extracted:

  * `title` ‚Üí From `h3 > a` attribute.
  * `price` ‚Üí From `<p class="price_color">`.
  * `rating` ‚Üí From class attribute.

---

## 3Ô∏è‚É£ Store Data in CSV

```python
df = pd.DataFrame({"Title": titles, "Price": prices})
df.to_csv("books_multi_page.csv", index=False)
print("Saved multi-page data to books_multi_page.csv")
```

* Convert extracted lists into DataFrame.
* Save **Title + Price** into CSV file.
* `index=False` ‚Üí Don‚Äôt save row numbers.

---

## 4Ô∏è‚É£ Small Practice Task üöÄ

```python
df = pd.DataFrame({"Title": titles, "Price": prices, "Rating": ratings})
df.to_csv("books_full.csv", index=False)
print("Saved multi-page data to books_full.csv")
```

* Create another DataFrame with `Rating` column.
* Save complete data (`Title + Price + Rating`) into CSV.

---

## 5Ô∏è‚É£ Summary

* Used **requests + BeautifulSoup** to scrape multiple pages.
* Extracted **title, price, and rating** from each book.
* Stored scraped data into **DataFrame**.
* Saved data into **CSV files** (`books_multi_page.csv`, `books_full.csv`).
* **Key Functions**: `.format()`, `requests.get()`, `soup.find_all()`, `.to_csv()`.
