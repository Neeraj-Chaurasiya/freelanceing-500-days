# LEARNING DATA ANALYST STEP-BY-STEP.

# üìÖ Day 1 ‚Äì NumPy Basics

## ‚úÖ Topics Covered
- Introduction to NumPy
- Creating arrays from Python lists
- Array attributes: `type`, `shape`
- Special arrays: `zeros`, `ones`, `eye`, `arange`, `linspace`
- Indexing & slicing (1D and 2D)

## üéØ Practice Task
1. Create an array with numbers 1‚Äì20.  
2. Print only even numbers.  
3. Create a 4x5 matrix and print:  
   - 2nd row  
   - 3rd column  
   - Last element

---

# üìÖ Day 2 ‚Äì NumPy Operations & Matrix Handling

## ‚úÖ Topics Covered
- Arithmetic operations on arrays (`+`, `-`, `*`, `/`, `**`)
- Mathematical & statistical functions:
  - `min`, `max`, `mean`, `median`, `std`, `sum`
- Reshape and axis-based operations
  - `reshape()`
  - `sum(axis=0)`, `sum(axis=1)`
- Random number generation
  - `np.random.rand`, `np.random.randint`, `np.random.seed`

## üéØ Practice Task
1. Create a 3x3 random integer matrix (values 1‚Äì50).  
2. Print row-wise maximum and column-wise minimum.  
3. Create a 1D array (1‚Äì16), reshape to 4x4, then print:  
   - Row-wise sum  
   - Column-wise sum  

---

# üìÖ Day 3 ‚Äì Pandas Basics (Series & DataFrame)

## ‚úÖ Topics Covered
- Introduction to **pandas** library  
- **Series (1D data)**  
  - Creating from list  
  - Custom index  
- **DataFrame (2D data)**  
  - Creating from dictionary  
  - Rows & columns overview  
- Basic DataFrame functions  
  - `head()`, `tail()`, `shape`, `info()`, `describe()`

## üéØ Practice Task
1. Create a student DataFrame with 5 students ‚Üí Columns: `Name, Age, Marks, City`.  
2. Print first 3 rows (`head(3)`).  
3. Check DataFrame shape and info.  
4. Calculate `Marks` ka mean aur max value.  

---

# üìÖ Day 4 ‚Äì Pandas Indexing, Selection & Filtering

## ‚úÖ Topics Covered
- **Column selection**
  - `df["col"]` ‚Üí single column (Series)
  - `df[["col1", "col2"]]` ‚Üí multiple columns (DataFrame)
- **Row selection**
  - `iloc` ‚Üí index-based (0,1,2‚Ä¶)
  - `loc` ‚Üí label-based (row labels, specific columns)
- **Conditional filtering**
  - `df[df["Marks"] > 88]`
  - `df[(df["Age"] > 22) & (df["Marks"] >= 88)]`
- **Adding & dropping columns**
  - `df["new_col"] = ...`
  - `df.drop("col", axis=1)`

## üéØ Practice Task
1. Ek **employee DataFrame** banao with columns: `EmpID, Name, Age, Department, Salary` (5 employees).  
2. Sirf `Name` aur `Salary` column print karo.  
3. Sirf un employees ko print karo jinki `Salary > 50,000` ho.  
4. Ek naya column **"Bonus"** add karo (Salary ka 10%).  
5. `"Department"` column drop karo.  

---

# üìÖ Day 5 ‚Äì Pandas GroupBy, Aggregate & Pivot Table

## ‚úÖ Topics Covered
- **GroupBy basics**
  - `df.groupby("col")["target"].mean()`
- **Multiple aggregations**
  - `agg(["mean","max","min"])`
- **GroupBy with multiple columns**
  - `df.groupby(["col1","col2"])["target"].sum()`
- **Pivot tables**
  - `df.pivot_table(index="col", values="target", aggfunc="mean")`

## üéØ Practice Task
1. Ek **sales DataFrame** banao with columns: `Region, Product, Sales, Profit` (8 rows).  
2. Group by `Region` ‚Üí total **Sales** nikalo.  
3. Group by `Product` ‚Üí average **Profit** nikalo.  
4. Ek **Pivot Table** banao jisme:
   - Rows = Region  
   - Columns = Product  
   - Values = Sales sum  

---

# üìÖ Day 6 ‚Äì Pandas Merge, Join & Concat

## ‚úÖ Topics Covered
- **Concatenation**
  - `pd.concat([df1, df2])` ‚Üí vertical (rows) combine
  - `pd.concat([df1, df2], axis=1)` ‚Üí horizontal (columns) combine
- **Merge (SQL JOIN style)**
  - `pd.merge(df1, df2, on="col", how="inner")`
- **Different Types of Joins**
  - `how="left"` ‚Üí Left Join
  - `how="right"` ‚Üí Right Join
  - `how="outer"` ‚Üí Full Outer Join
- **Join using Index**
  - `df1.join(df2, how="outer")`

## üéØ Practice Task
1. Ek **students DataFrame** banao with columns:  
   `RollNo, Name, Class`.  
2. Ek **marks DataFrame** banao with columns:  
   `RollNo, Subject, Marks`.  
3. Merge karke ek final DataFrame banao jisme:  
   `Student Name + Subject + Marks` ho.  
4. Concatenate karke do alag batches ka students data ek hi DataFrame me lao.  

---

# üìÖ Day 7 ‚Äì Pandas GroupBy & Aggregation

## ‚úÖ Topics Covered
- **Basic GroupBy**
  - `df.groupby("col")["val"].mean()` ‚Üí group-wise average
- **Multiple Aggregations**
  - `agg(["mean","sum","max","min"])` ‚Üí ek group ke multiple stats
- **GroupBy on Multiple Columns**
  - `df.groupby(["col1","col2"])["val"].mean()`
- **Sorting Aggregated Results**
  - `.sort_values(ascending=False)`

## üéØ Practice Task
1. Ek **sales DataFrame** banao jisme columns ho:  
   `Region, Product, Sales`.  
2. `groupby()` karke har **Region ka total sales** nikaalo.  
3. Har **Product ka average sales** nikaalo.  
4. Top 1 product identify karo jo sabse zyada bikta hai.  

---

# üìÖ Day 8 ‚Äì Pandas Data Cleaning & Preprocessing

## ‚úÖ Topics Covered
- **Missing Values**
  - `df.isnull()` ‚Üí missing check
  - `df.dropna()` ‚Üí drop missing rows
  - `df.fillna(value)` ‚Üí fill missing values
- **Duplicates**
  - `df.duplicated()` ‚Üí duplicate check
  - `df.drop_duplicates()` ‚Üí remove duplicates
- **Rename + Replace**
  - `df.rename(columns={"old":"new"})`
  - `df["col"].replace(old, new)`
- **String Cleaning**
  - `.str.strip()` ‚Üí trim spaces  
  - `.str.upper()` ‚Üí uppercase  
  - `.str.title()` ‚Üí title case  

## üöÄ Practice Task
1. Ek DataFrame banao jisme:
   - Kuch `NaN` values
   - Duplicate rows
   - Extra spaces in strings  
2. Data cleaning steps apply karke ek **final clean dataset** banao.  
3. GitHub pe upload karo: **pandas_cleaning_day8.ipynb**  

---

## ‚úÖ Expected Output
Tumhe real-world **data cleaning pipeline** samajh aayega:
- Missing values handle karna
- Duplicates remove karna
- Columns rename/replace
- String strip/title/upper  

‚ö° Ye freelancing aur industry projects me sabse zyada use hoti skill hai.  

---

# üìÖ Day 9 ‚Äì Exploratory Data Analysis (EDA ‚Äì Part 1)

## ‚úÖ Topics Covered
- **Dataset Load & Info**
  - `pd.read_csv()` ‚Üí dataset import
  - `.head()`, `.info()`, `.describe()` ‚Üí dataset samajhna
- **Univariate Analysis**
  - `.mean()`, `.value_counts()`, `.unique()` ‚Üí single column analysis
- **Bivariate Analysis**
  - `.groupby()` ‚Üí relation between two columns
- **Visualization (Matplotlib Basics)**
  - Histogram ‚Üí distribution
  - Bar Plot ‚Üí average comparison
  - Scatter Plot ‚Üí relation between numeric columns

## üöÄ Practice Task
Dataset: **tips.csv**  
1. Find **highest tip kis day pe di gayi**  
2. Calculate **Male vs Female average bill difference**  
3. Scatter plot banao: `size` vs `total_bill`  

Upload: **eda_day9.ipynb** to GitHub.

---

## ‚úÖ Expected Output
- Dataset ka **overview** samajh aayega  
- Univariate + Bivariate analysis clear hoga  
- Basic **EDA plots** (histogram, bar, scatter) seekh jaoge  

‚ö° Ye har Kaggle project aur freelancing analysis ka **Step-1** hai.  

---

# üìÖ Day 10 ‚Äì Exploratory Data Analysis (EDA ‚Äì Part 2, Seaborn)

## ‚úÖ Topics Covered
- **Seaborn Setup**
  - `sns.histplot`, `sns.boxplot` ‚Üí distribution + outliers
  - `sns.barplot`, `sns.countplot` ‚Üí categorical comparison
  - `sns.scatterplot`, `sns.lineplot` ‚Üí relationships
  - `sns.heatmap` ‚Üí correlation matrix

## üìä Visualizations
1. **Distribution Plots**
   - Histogram (total_bill)
   - Boxplot (bill per day)
2. **Categorical Plots**
   - Barplot (avg bill by day)
   - Countplot (gender count)
3. **Relationship Plots**
   - Scatterplot (bill vs tip, hue=sex)
   - Lineplot (bill by group size)
4. **Heatmap**
   - Correlation matrix

## üöÄ Practice Task
Dataset: **tips.csv**  
1. Male vs Female **average tip** ka barplot banao.  
2. Day-wise **bill distribution** ka boxplot banao.  
3. Heatmap banao aur identify karo ‚Üí **tip ke saath sabse zyada correlated column** kaunsa hai.  

---

## ‚úÖ Expected Output
- Distribution, categorical, relationship aur heatmap plots ka mastery.  
- Gender, day aur correlation-based analysis kar paoge.  
- EDA ke visualization step me confidence aayega üöÄ  


# üìÖ Day 11 ‚Äì Mini Project: Sales Analysis

## ‚úÖ Topics Covered
- Dataset Creation / Import (Superstore-style sales data)
- Data Cleaning:
  - Missing values check
  - Duplicate handling
  - String/category cleanup
- Exploratory Data Analysis:
  - Revenue calculation
  - Top-selling product
  - Region-wise sales
- Visualization (Seaborn + Matplotlib):
  - Product-wise sales (bar plot)
  - Region-wise sales (bar plot)
  - Correlation heatmap

---

## üöÄ Practice Task
1. Har **Region ka total revenue** nikalo.  
2. Identify karo **sabse zyada revenue generate karne wala product**.  
3. Ek **Bar Chart** banao ‚Üí Product vs Revenue.  
4. Ek **Pie Chart** banao ‚Üí Region-wise sales percentage.  

---

## ‚úÖ Expected Output
- Tum ek **Mini Sales Analysis Project** complete kar loge.  
- Portfolio ke liye ek **freelancing-style analysis notebook** ready hoga.  
- Real-world workflow (Data Cleaning ‚Üí EDA ‚Üí Visualization ‚Üí Insights) clear ho jayega üöÄ  


# üìÖ Day 12 ‚Äì Web Scraping with Python (Part 1)

## ‚úÖ Topics Covered
- **Libraries for Scraping**
  - `requests` ‚Üí website ka HTML fetch karna
  - `BeautifulSoup` ‚Üí HTML parse & extract
  - `pandas` ‚Üí scraped data ko CSV me save karna
- **Basic Steps**
  - Send request ‚Üí `requests.get(url)`
  - Parse HTML ‚Üí `BeautifulSoup(html, "html.parser")`
  - Extract elements ‚Üí `find_all("tag")`
  - Save to CSV with pandas

---

## üöÄ Practice Task
1. Website: **books.toscrape.com**  
   - Scrape book **title**, **price**, **rating**  
2. Data ko **DataFrame** me save karo  
3. Export CSV: **books_prices.csv**  
4. Upload on GitHub: **web_scraping_day12.ipynb**

---

## ‚úÖ Expected Output
- Tum ek **basic web scraper** bana paoge.  
- Titles, prices, ratings jaisa **structured dataset** extract kar loge.  
- Freelancing ke liye ekdum useful skill (clients aksar data scraping mangte hain).  
- Next step: Pagination & multiple pages scrape karna (Day-13). üöÄ  


# üìÖ Day 13 ‚Äì Web Scraping with Python (Part 2 / Advanced)

## ‚úÖ Topics Covered

* **Advanced Scraping Concepts**

  * Pagination ‚Üí multiple pages scrape karna
  * Looping through URLs ‚Üí automate scraping
  * Extract multiple fields ‚Üí title, price, rating
  * Store results in **lists** ‚Üí then convert to DataFrame
* **Saving Data**

  * `pandas.DataFrame()` ‚Üí structured data
  * `to_csv()` ‚Üí export multiple-page data

---

## üöÄ Practice Task

1. Website: **books.toscrape.com**

   * Scrape **first 10 pages**
   * Extract: **title**, **price**, **rating**
2. Store all data in a **single DataFrame**
3. Export CSV: **books\_full.csv**
4. Upload on GitHub: **web\_scraping\_day13.ipynb**

---

## ‚úÖ Expected Output

* Tum ek **multi-page web scraper** bana paoge
* Titles, prices, ratings jaisa **complete dataset** extract kar loge
* Freelancing ke liye **ready-to-use dataset** milega
* Next step: Scraped data ko **Pandas / Excel me clean & analyze** karna (Day-14) üöÄ


# üìÖ Day 14 ‚Äì Scraped Data Cleaning & Analysis

## ‚úÖ Topics Covered

* **Data Loading**

  * Import scraped CSV into pandas DataFrame
* **Data Cleaning**

  * Remove currency symbols & convert Price ‚Üí numeric
  * Convert Rating (One‚ÄìFive) ‚Üí numeric scale (1‚Äì5)
  * Handle missing values / duplicates
* **Data Analysis**

  * Average price, highest priced book
  * Average rating
  * Grouping & aggregation
* **Visualization**

  * Price distribution (histogram)
  * Ratings count (bar chart)

---

## üöÄ Practice Task

1. Extract **Top 10 most expensive books**
2. Calculate **average price per rating** (1‚Äì5)
3. Create **bar chart** ‚Üí Rating vs Average Price
4. Export cleaned dataset ‚Üí **books\_cleaned.csv**
5. Upload notebook ‚Üí **web\_scraping\_day14.ipynb**

---

## ‚úÖ Expected Output

* Tum ek **scraped dataset ko clean & analyze** kar paoge
* Exported file: **books\_cleaned.csv** (ready for freelancing use)
* Clear **visual insights** (price trends, rating distribution)
* Next step (Day-15): **Automated Dashboard / Report (Excel / PowerBI)** üöÄ

# üìÖ Day 15 ‚Äì Automated Dashboard / Report

## ‚úÖ Topics Covered

* **Load Cleaned Dataset**

  * Import `books_cleaned.csv` into pandas DataFrame
* **Summary Metrics**

  * Total Books, Average Price, Average Rating
* **Visualizations**

  * Price Distribution (histogram)
  * Top 10 Expensive Books (bar chart)
  * Rating vs Average Price (bar chart)
* **Automated Excel Report**

  * Multiple sheets: All Books, Top 10 Expensive, Avg Price by Rating
  * Ready-to-share client deliverable

---

## üöÄ Practice Task

1. Display summary metrics: **Total Books, Average Price, Average Rating**
2. Plot: Price Distribution, Top 10 Expensive Books, Rating vs Average Price
3. Export **Excel report** with multiple sheets: `books_report.xlsx`
4. Upload notebook ‚Üí **web\_scraping\_day15\_dashboard.ipynb**

---

## ‚úÖ Expected Output

* Tum ek **automated dashboard / report** create kar paoge
* Cleaned dataset + plots + summary metrics **ek hi file / Excel report me ready**
* Freelancing ke liye **professional deliverable**
* Next step (Day-16): **Portfolio polishing + GitHub README / showcase ready** üöÄ


# üìÖ Day 16 ‚Äì Portfolio & GitHub Showcase

## ‚úÖ Topics Covered

* **Git & GitHub Basics**

  * Create repo ‚Üí `git init`
  * Track changes ‚Üí `git add .` + `git commit -m "message"`
  * Push to GitHub ‚Üí `git push origin main`

* **README.md Polish**

  * Har project ke liye description likho
  * Structure: Problem ‚Üí Approach ‚Üí Solution ‚Üí Output (screenshots / CSV / plots)

* **Portfolio Repo Structure**

  * Projects folder ke andar:

    * `Project_01_SalesAnalysis/`
    * `Project_02_WebScraping/`
    * `Project_03_Dashboard/`
  * Har project folder me ek `README.md` + code/data

* **GitHub Profile Setup**

  * Profile README banayo (intro + skills + contact)
  * Projects ko **pin** karo (Sales Analysis, Web Scraping, Dashboard)

---

## üöÄ Practice Task

1. GitHub profile me ek **professional repo structure** banao.
2. Har project ko **alag repo** me push karo.
3. Har repo ke liye ek **portfolio-style README.md** likho.
4. Profile README banao jisme tumhari **skills + projects + contact links** ho.

---

## ‚úÖ Expected Output

* Ek **polished GitHub profile** jo tumhara portfolio show kare.
* Recruiters/clients ko lage ki tumne **real projects** kiye hain.
* Freelancing aur internship ke liye tumhari profile **ready-to-share** hogi. üöÄ


# üìÖ Day 17 ‚Äì SQL Practice with SSMS (Sales Database)

## ‚úÖ Topics Covered

* **Database Creation**
  * SSMS me ek naya Database banao ‚Üí `SalesDB`
* **Table Creation**
  * Sales table with columns: id, product, category, price, quantity
* **Data Insertion**
  * Insert dummy product records (10‚Äì15 rows minimum)
* **Queries**
  * View all data (`SELECT *`)
  * Filtered data (`WHERE price > ...`)
  * Aggregation (`GROUP BY category ‚Üí total revenue`)
* **Screenshots for Notes**
  * Queries + Outputs capture karke Day-17 notes.md me daalo

---

## üöÄ Practice Task

1. Create **SalesDB** database in SSMS  
2. Insert at least **10‚Äì15 sample product records**  
3. Run queries:
   * View all data
   * Filter products by price
   * Category-wise revenue calculation  
4. Take **screenshots of results** and add them in `Day-17/notes.md`  
5. Upload to GitHub ‚Üí portfolio roadmap continue karo üöÄ

---

## ‚úÖ Expected Output

* Tum ek **Sales Database** SSMS me successfully bana loge  
* Basic SQL commands practice karoge: **CREATE, INSERT, SELECT, WHERE, GROUP BY**  
* Screenshots ke saath tumhare GitHub roadmap me ek **solid SQL practice proof** add ho jayega  
* Freelancing / interview ke liye tumhe **SQL confidence boost** milega ‚úÖ


# üìÖ Day 18 ‚Äì SQL Intermediate (Joins & Aggregations)

## ‚úÖ Topics Covered

* **Customers Table Creation**
  * Ek alag table banakar customer details store karna  
* **Linking Tables**
  * `ALTER TABLE` se Sales aur Customers ko connect karna (via `customer_id`)  
* **Joins**
  * **INNER JOIN** ‚Üí Sales + Customers ka matching data
  * **LEFT JOIN** ‚Üí Sabhi customers ke sath unke sales records
* **Advanced Queries**
  * City-wise revenue calculation
  * Top 3 highest priced products

---

## üìÇ Database Schema

**sales**  
- id  
- product  
- category  
- price  
- quantity  
- customer_id  

**customers**  
- customer_id  
- name  
- city  

---

## üöÄ Practice Task

1. `customers` table banao aur dummy records insert karo  
2. `sales` table ko update karo ‚Üí `customer_id` ke sath link karo  
3. Queries run karo:
   * INNER JOIN ‚Üí Sales + Customers ka combined data
   * City-wise revenue calculation  
   * Top 3 highest priced products  
4. Queries aur unke **screenshots** Day-18 `notes.md` me add karo  
5. Upload karo GitHub pe:  
   - `day18_sql_practice.sql` ‚Üí queries  
   - `notes.md` ‚Üí explanation + screenshots  
   - `README.md` ‚Üí portfolio summary  

---

## ‚úÖ Expected Output

* Tum **Joins (INNER + LEFT)** ka use samjhoge  
* City-wise & customer-wise revenue calculate kar paoge  
* Advanced filtering aur top-N queries likhna seekh jaoge  
* Freelancing clients ke liye **customer + sales linked reports** generate karna easy ho jayega  
* Portfolio me ek aur **professional SQL project** add ho jayega üöÄ


# üìÖ Day 19 ‚Äì SQL Advanced Filtering & Functions

## ‚úÖ Topics Covered

* **Advanced Filtering**
  * `WHERE` clause with conditions
  * `BETWEEN`, `IN`, `LIKE`, `IS NULL`
* **ORDER BY**
  * Multiple columns ke sath sorting (ASC / DESC)
* **SQL Functions**
  * **String Functions** ‚Üí `LEN()`, `UPPER()`, `LOWER()`, `SUBSTRING()`
  * **Date Functions** ‚Üí `GETDATE()`, `YEAR()`, `MONTH()`
  * **Numeric Functions** ‚Üí `ROUND()`, `ABS()`

---

## üöÄ Practice Task

1. **Filtering Queries**
   * Price **1000 aur 50000 ke beech**
   * Product only `Laptop` ya `Phone`
   * Customers jinka naam `N` se start hota ho
   * Customers jinki **sales NULL** hai  

2. **Functions Practice**
   * Customer name length ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
   * Product ko `UPPERCASE` me dikhana
   * Price ko round off karna (nearest hundred)
   * Current date + Year extract karna  

3. **ORDER BY**
   * City wise ascending + Name wise descending sort  

4. Queries run karke screenshots `notes.md` me daalo  
5. Upload GitHub pe:
   - `day19_sql_practice.sql` ‚Üí queries
   - `notes.md` ‚Üí explanation + screenshots
   - `README.md` ‚Üí portfolio roadmap summary  

---

## ‚úÖ Expected Output

* Tum **filtering** ka mastery kar loge  
* **String, Date, Numeric functions** ka real-world use samjhoge  
* Data ko **clean & formatted** way me dikhana seekh jaoge  
* Freelancing ke liye client ke reports aur formatted SQL outputs banane me expert ban jaoge üöÄ


# üìÖ Day 20 ‚Äì SQL Aggregations & Subqueries

## ‚úÖ Topics Covered

* **Aggregate Functions**
  * `SUM()`, `AVG()`, `COUNT()`, `MAX()`, `MIN()`
* **GROUP BY**
  * Category / City wise totals
* **HAVING**
  * Aggregated groups pe condition lagana
* **Subqueries**
  * Ek query ke andar dusri query (nested queries)

---

## üöÄ Practice Task

1. **Aggregate Functions**
   * Total revenue (`SUM`)
   * Average product price (`AVG`)
   * Number of customers (`COUNT`)
   * Max & Min product price (`MAX`, `MIN`)

2. **GROUP BY + HAVING**
   * City-wise total revenue calculate karo
   * Sirf wahi cities show karo jinka revenue **50,000 se zyada** ho

3. **Subqueries**
   * Products jo **average price se zyada costly** hain
   * Customers jinhone **Laptop purchase kiya** hai

4. Queries run karke screenshots `notes.md` me daalo  
5. Upload GitHub pe:
   - `day20_sql_practice.sql` ‚Üí queries
   - `notes.md` ‚Üí explanation + screenshots
   - `README.md` ‚Üí portfolio roadmap summary  

---

## ‚úÖ Expected Output

* Tum **aggregation reporting** kar paoge  
* **City/Product wise summaries** easily generate kar paoge  
* **Subqueries** ki help se complex problems solve karna aa jayega  
* Freelancing aur interviews ke liye **professional SQL reporting skills** develop ho jayegi üöÄ


# üìÖ Day 21 ‚Äì SQL Views, Indexes & Stored Procedures

## ‚úÖ Topics Covered
* **Views** ‚Üí Reusable queries banane ke liye  
* **Indexes** ‚Üí Query performance fast karne ke liye  
* **Stored Procedures** ‚Üí Queries ko automate karne ke liye  

---

## üöÄ Practice Queries

### üîπ Views
* City-wise revenue ka ek **View** banao aur use query me reuse karo

### üîπ Indexes
* Customers table ke `City` column pe index add karo  
* Sales table ke `Product` column pe index add karo  

### üîπ Stored Procedures
* Ek stored procedure banao jo **Top N products by revenue** return kare  
* Procedure ko execute karke output verify karo  

---

## üìÇ Files
* `day21_queries.sql` ‚Üí saare SQL queries  
* `outputs/` ‚Üí screenshots ya exported CSVs  

---

## ‚úÖ Expected Output
* Tum ek **View** bana paoge jo multiple reports me reuse hoga  
* **Indexes** ki wajah se queries fast execute hongi  
* **Stored Procedure** se tum automated reports generate kar paoge  

---

## üåü Portfolio Value
‚úî Adds **professional database skills** (freelancing & jobs me high demand)  
‚úî Shows you can **optimize queries** with indexes  
‚úî Proof that you can **automate reporting** with stored procedures üöÄ  


# üìÖ Day 22 ‚Äì SQL Advanced Joins, Nested Queries & Constraints

## ‚úÖ Topics Covered
* **Self JOIN** ‚Üí ek hi table ke andar join  
* **FULL OUTER JOIN** ‚Üí dono tables ka complete data  
* **Nested Subqueries** ‚Üí query ke andar query  
* **Constraints** ‚Üí data ko validate karne ke liye  
  - NOT NULL  
  - UNIQUE  
  - CHECK  
  - DEFAULT  

---

## üöÄ Practice Tasks

### üîπ Self JOIN
* Customers table me **self join** karke same city ke customers find karo  

### üîπ FULL OUTER JOIN
* Sales aur Customers ka **full outer join** karke dono tables ka data combine karo  

### üîπ Nested Subqueries
* Customers identify karo jinhone **average price se zyada ke products** kharide hain  

### üîπ Constraints
* Ek `Employees` table banao with constraints:  
  - `NOT NULL` ‚Üí Name  
  - `UNIQUE` ‚Üí Email  
  - `CHECK` ‚Üí Age >= 18  
  - `DEFAULT` ‚Üí City = 'Unknown'  

---

## üìÇ Files
* `day22_queries.sql` ‚Üí saare SQL queries  
* `outputs/` ‚Üí screenshots ya exported CSVs  

---

## ‚úÖ Expected Output
* Tum **advanced joins** use karke complex data analysis kar paoge  
* **Nested queries** se complex reports generate kar paoge  
* **Constraints** se tum data integrity maintain karna seekh jaoge  

---

## üåü Portfolio Value
‚úî Proof ki tum **complex joins** aur **subqueries** handle kar sakte ho  
‚úî Freelancing ke liye **data validation + integrity rules** apply karna aata hai  
‚úî Adds strong SQL concepts (industry me high demand) üöÄ  


# üìÖ Day 23 ‚Äì SQL Aggregations & Window Functions

## üìå Overview
Day-23 me maine **Advanced SQL Reporting & Analytics** cover kiya jisme aggregation aur window functions use karke professional level queries banayi.  
Ye skills freelancing, data analytics, aur real-world reporting ke liye directly useful hain.  

---

## ‚úÖ Topics Mastered
- **Aggregate Functions** ‚Üí SUM(), AVG(), COUNT(), MIN(), MAX()  
- **GROUP BY** ‚Üí category / city wise data grouping  
- **HAVING** ‚Üí aggregated data par conditions lagana  
- **Window Functions**  
  - ROW_NUMBER() ‚Üí row-wise numbering  
  - RANK() ‚Üí ranking based on sales  
  - PARTITION BY ‚Üí customer/product wise segmentation  
  - AVG() OVER() ‚Üí overall average ke sath comparison  


# üìÖ Day 25 ‚Äì SQL Correlated Subqueries & Conditional Logic

## üìå Overview
Day-25 me maine **advanced SQL querying techniques** seekhe jo **dynamic reports aur business insights** ke liye use hote hain.  
Isme focus tha **Correlated Subqueries** (row-by-row comparison) aur **CASE WHEN** (conditional reporting).  

---

## ‚úÖ Topics Mastered
- **Correlated Subqueries** ‚Üí Subquery jo har row ke liye outer query ke context me run hoti hai  
- **CASE WHEN (Conditional Logic)** ‚Üí Queries me IF-ELSE type decision making  
- **Subquery + CASE combo** ‚Üí Smart reporting & business insights ke liye powerful approach


# üìÖ Day 26 ‚Äì SQL Transactions & ACID Properties

## üìå Overview
Day-26 me maine **transactions aur ACID principles** master kiye, jo **financial systems, ecommerce aur banking applications** ke liye backbone hote hain.  
Transactions ensure karte hain ki multiple operations **ya to saath me complete ho ya cancel ho jaye** ‚Üí data hamesha safe aur consistent rahta hai.  

---

## ‚úÖ Topics Mastered
- **Transactions (BEGIN, COMMIT, ROLLBACK)** ‚Üí All-or-Nothing execution  
- **ACID Properties**  
  - Atomicity ‚Üí Sabhi steps execute ho ya koi nahi  
  - Consistency ‚Üí Database valid state me rahe  
  - Isolation ‚Üí Parallel transactions ek dusre ko disturb na kare  
  - Durability ‚Üí Commit ke baad data permanent  
- **Savepoints** ‚Üí Transactions ke beech checkpoints  
- **Isolation Levels** ‚Üí Transactions ke concurrency control  

---

# üìÖ Day 27 ‚Äì SQL Triggers (AFTER & INSTEAD OF)

## üìå Overview
Day-27 me maine **SQL Triggers** master kiye.  
Triggers wo **automatic actions** hote hain jo INSERT, UPDATE, DELETE par run hote hain.  
Isse databases khud apne aap **audit logs maintain**, **data validation**, aur **auto-calculations** kar sakte hain ‚Äì bina manual code likhe.

---

## ‚úÖ Topics Mastered
- **Trigger Basics** ‚Üí Auto execution on data changes  
- **AFTER Triggers** ‚Üí Action hone ke baad run hote hain (e.g. audit logs)  
- **INSTEAD OF Triggers** ‚Üí Original action replace hota hai custom logic se  
- **Use Cases** ‚Üí  
  - Audit trails maintain karna  
  - Automatic calculations  
  - Data validation & restrictions  

---

# üìÖ Day 28 ‚Äì SQL User Roles & Permissions (Database Security)

## üìå Overview
Day-28 me maine **SQL Database Security** ke core concepts master kiye.  
Ab mai samjh gaya hoon kaise database me **users, roles aur permissions** control karte hain ‚Äî  
taaki **unauthorized access** aur **data misuse** na ho.

---

## ‚úÖ Topics Mastered
- **Database Security Basics** ‚Üí Users, Roles, Permissions ka concept  
- **User Management** ‚Üí Login aur Database User creation  
- **Roles** ‚Üí Predefined roles (db_datareader, db_datawriter, db_owner)  
- **Permissions Control** ‚Üí GRANT, REVOKE, DENY commands  
- **Practical Use Cases** ‚Üí  
  - Clients ko **read-only access** dena  
  - Developers ke liye **restricted editing rights**  
  - Database safety aur compliance maintain karna  

---

# üìÖ Day 29 ‚Äì SQL Optimization & Query Performance

## ‚úÖ Topics Covered
- **Query Execution Plan (EXPLAIN / SHOWPLAN)** ‚Üí Query engine ka behavior samajhna  
- **Indexes (Clustered vs Non-Clustered)** ‚Üí Fast data retrieval ke liye indexing  
- **Performance Analysis** ‚Üí Execution time aur plan comparison  
- **Optimization Techniques**  
  - Efficient **JOINs** vs **Subqueries**  
  - Use of **CTEs** for better readability  
  - Avoiding unnecessary SELECT *  
  - Dropping unused indexes for write optimization  
- **Composite Indexes** ‚Üí Multi-column search optimization  
- **Real-time Performance Testing** using `SET STATISTICS TIME`  
- **Best Practices** ‚Üí Query structure tuning for scalability  

---

## üåü Portfolio Value
- ‚úÖ Demonstrates **Advanced SQL Performance Optimization** expertise  
- ‚úÖ Proof that you can **analyze execution plans** and make queries faster  
- ‚úÖ Shows capability to handle **large-scale datasets efficiently**  
- ‚úÖ Adds **high-value technical depth** ‚Äî demanded by top companies (Google, Amazon, Flipkart, etc.)  
- ‚úÖ Freelancing edge: build **faster dashboards & analytics systems** for clients  
- ‚úÖ Reinforces you as a **professional SQL developer** who understands backend performance tuning  

---

# üìÖ Day 30 ‚Äì Mini Project: E-Commerce Database Analysis (End-to-End SQL Case Study)

## üéØ Objective
Build a **real-world SQL Case Study** on *E-Commerce Sales Data* to perform  
business analysis using **advanced SQL concepts** like joins, subqueries, views, procedures, and triggers.

---

## ‚úÖ Topics Covered
| Concept | Description |
|----------|-------------|
| **Database Design** | Created tables for `Customers`, `Products`, and `Sales` |
| **Joins** | Combined data across tables for analysis |
| **Aggregations** | Calculated revenue, top products, category-wise sales |
| **Subqueries** | Used for comparative data logic |
| **Views** | Created reusable summary queries (Customer Sales Summary) |
| **Stored Procedures** | Automated city-wise sales reports with parameters |
| **Triggers** | Added automation to log/notify after inserts |
| **Filtering & Grouping** | Analyzed sales within date ranges and categories |
| **Portfolio-Ready Project** | Complete dataset + insights + automation setup |

üß† Concepts Reinforced

‚úÖ Relational Database Design

‚úÖ Data Aggregation & Filtering

‚úÖ Views & Procedures for Reusability

‚úÖ Automation with Triggers

‚úÖ Real-world Query Logic & Reporting

üíº Final Outcome

By the end of Day 30, you will have:

A fully functional SQL project simulating an e-commerce analytics system

Skills in real-world database automation and reporting

A portfolio-ready GitHub project demonstrating your SQL proficiency

Resume line to add:

‚ÄúBuilt an end-to-end SQL case study analyzing e-commerce sales using advanced database concepts (joins, procedures, triggers, and performance optimization).‚Äù

üåü Portfolio Value

‚úî Demonstrates end-to-end SQL capability (design ‚Üí logic ‚Üí automation)
‚úî Looks impressive to recruiters and freelance clients
‚úî Perfect for Data Analyst / SQL Developer interviews
‚úî Adds real project proof to your GitHub profile üöÄ

--

# üìÖ Day 31 ‚Äì Python + SQL Integration (Data Fetching & Analysis)

## üéØ Objective
Connect **SQL Server** with **Python (pandas)** to fetch, analyze, and export real business data.  
Ye freelancing aur data analytics projects ka sabse common real-world workflow hai üíº  

---

## ‚úÖ Topics Covered
- **Database Connection (pyodbc)** ‚Üí Secure connection between Python & SQL Server  
- **Data Extraction (SQL Queries)** ‚Üí Fetch live data directly into Python  
- **Data Conversion (pandas)** ‚Üí Convert query result into a DataFrame  
- **Data Analysis**  
  - Customer-wise revenue summary  
  - Product/category performance  
- **Excel Export (openpyxl)** ‚Üí Save insights as client-ready reports  
- **Integration Workflow** ‚Üí SQL + Python combo for automation & dashboards  

---

## üß© Pre-Setup
- ‚úÖ SSMS me `ECommerceDB` database ready (Day-30 project se)  
- ‚úÖ Python environment active (VS Code / venv)  
- ‚úÖ Dependencies install:
  ```bash
  pip install pyodbc pandas openpyxl

---

# üìÖ Day 32 ‚Äì Data Analysis & Visualization using SQL Data (Python + Pandas + Matplotlib)

## üéØ Objective
SQL se fetched data ko Python me analyze, clean, aur visualize karna.  
Ye freelancing aur data analyst jobs dono ke liye **core, real-world skill** hai.  

---

## ‚úÖ Topics Covered
- **Data Cleaning using pandas** ‚Üí Missing values, duplicates, and structure check  
- **Aggregation & Grouping** ‚Üí City-wise, Category-wise revenue analysis  
- **Visualization using Matplotlib & Seaborn** ‚Üí Interactive charts for insights  
- **Data Export** ‚Üí Cleaned + visualized reports exported to Excel  
- **SQL-Python Integration** ‚Üí Combining database logic with analytical visualization  

---

# üìÖ Day 33 ‚Äì Power BI Dashboard (Part 1): Connect & Model Data

## üéØ Objective
Learn how to connect **SQL Server Database (ECommerceDB)** to **Power BI**, clean and model data for dashboard creation.  
Ye foundation hai ek **dynamic business dashboard** banane ke liye (real-world client style setup). üíº  

---

## ‚úÖ Topics Covered
- **Power BI Interface Overview**  
- **SQL Server Connection** ‚Äì Import live data from SQL  
- **Data Modeling** ‚Äì Define relationships between tables  
- **Data Cleaning (Power Query Editor)** ‚Äì Rename, remove, format & transform columns  
- **Calculated Columns** ‚Äì Add business logic (e.g., Total Sales = Quantity √ó Price)  
- **Data Model Validation** ‚Äì Verify correct relationships between tables  

---

## üß© Pre-Setup
- üü¢ Install Power BI Desktop  
  üëâ Download: [https://powerbi.microsoft.com/desktop](https://powerbi.microsoft.com/desktop)  
- üü¢ Ensure your SQL Server Database `ECommerceDB` is ready (from Day 30)  
- üü¢ Tables required: **Customers**, **Products**, **Sales**  

---

# üìÖ Day 34 ‚Äì Power BI Dashboard (Part 2): Visual Design & Insights

## üéØ Objective
SQL Server se imported data ka use karke **Power BI me interactive dashboard** banana.  
Dashboard me include honge:  
- **Revenue by City**
- **Top Products**
- **Category-wise Sales Trends**
- **Key KPIs (Revenue, Sales, Customers)**  

Ye tumhara **first professional Power BI dashboard project** hoga ‚Äî  
jo **GitHub + LinkedIn portfolio** ke liye perfect hai üíº  

---

## ‚úÖ Topics Covered
- **Creating Visuals in Power BI**
  - Bar, Pie, Line, and Column charts
- **Revenue & Product Performance**
  - Total Revenue, Top Products, Category Sales
- **KPI Cards**
  - Total Revenue, Total Sales, Unique Customers
- **Interactive Filters (Slicers)**
  - City, Product Category, Date Range
- **Dashboard Formatting & Theme**
  - Consistent colors, alignment, title text & layout design  

---

# üìÖ Day 35 ‚Äì Power BI Project Export & Portfolio Upload

## üéØ Objective
Apna Power BI dashboard (from **Day 34**) final polish, export, aur **GitHub + LinkedIn** par upload karna.  
Isse tumhara **first professional portfolio project** ready ho jaayega:  
> üß† ‚ÄúE-Commerce Sales Dashboard using Power BI + SQL‚Äù

---

## ‚úÖ Topics Covered
- **Power BI Export Formats**
  - `.PBIX` (editable dashboard file)
  - `.PDF` (for portfolio presentation)
  - `.PNG` (dashboard snapshot for GitHub/LinkedIn)
- **GitHub Project Setup**
  - Folder structure and `README.md`
  - Upload final project files and screenshots
- **LinkedIn Post Preparation**
  - How to present your dashboard project professionally
- **(Optional)** Power BI Service Publish (cloud sharing)

---