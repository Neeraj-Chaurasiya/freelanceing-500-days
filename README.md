# LEARNING DATA ANALYST STEP-BY-STEP.

# ðŸ“… Day 1 â€“ NumPy Basics

## âœ… Topics Covered
- Introduction to NumPy
- Creating arrays from Python lists
- Array attributes: `type`, `shape`
- Special arrays: `zeros`, `ones`, `eye`, `arange`, `linspace`
- Indexing & slicing (1D and 2D)

## ðŸŽ¯ Practice Task
1. Create an array with numbers 1â€“20.  
2. Print only even numbers.  
3. Create a 4x5 matrix and print:  
   - 2nd row  
   - 3rd column  
   - Last element

---

# ðŸ“… Day 2 â€“ NumPy Operations & Matrix Handling

## âœ… Topics Covered
- Arithmetic operations on arrays (`+`, `-`, `*`, `/`, `**`)
- Mathematical & statistical functions:
  - `min`, `max`, `mean`, `median`, `std`, `sum`
- Reshape and axis-based operations
  - `reshape()`
  - `sum(axis=0)`, `sum(axis=1)`
- Random number generation
  - `np.random.rand`, `np.random.randint`, `np.random.seed`

## ðŸŽ¯ Practice Task
1. Create a 3x3 random integer matrix (values 1â€“50).  
2. Print row-wise maximum and column-wise minimum.  
3. Create a 1D array (1â€“16), reshape to 4x4, then print:  
   - Row-wise sum  
   - Column-wise sum  

---

# ðŸ“… Day 3 â€“ Pandas Basics (Series & DataFrame)

## âœ… Topics Covered
- Introduction to **pandas** library  
- **Series (1D data)**  
  - Creating from list  
  - Custom index  
- **DataFrame (2D data)**  
  - Creating from dictionary  
  - Rows & columns overview  
- Basic DataFrame functions  
  - `head()`, `tail()`, `shape`, `info()`, `describe()`

## ðŸŽ¯ Practice Task
1. Create a student DataFrame with 5 students â†’ Columns: `Name, Age, Marks, City`.  
2. Print first 3 rows (`head(3)`).  
3. Check DataFrame shape and info.  
4. Calculate `Marks` ka mean aur max value.  

---

# ðŸ“… Day 4 â€“ Pandas Indexing, Selection & Filtering

## âœ… Topics Covered
- **Column selection**
  - `df["col"]` â†’ single column (Series)
  - `df[["col1", "col2"]]` â†’ multiple columns (DataFrame)
- **Row selection**
  - `iloc` â†’ index-based (0,1,2â€¦)
  - `loc` â†’ label-based (row labels, specific columns)
- **Conditional filtering**
  - `df[df["Marks"] > 88]`
  - `df[(df["Age"] > 22) & (df["Marks"] >= 88)]`
- **Adding & dropping columns**
  - `df["new_col"] = ...`
  - `df.drop("col", axis=1)`

## ðŸŽ¯ Practice Task
1. Ek **employee DataFrame** banao with columns: `EmpID, Name, Age, Department, Salary` (5 employees).  
2. Sirf `Name` aur `Salary` column print karo.  
3. Sirf un employees ko print karo jinki `Salary > 50,000` ho.  
4. Ek naya column **"Bonus"** add karo (Salary ka 10%).  
5. `"Department"` column drop karo.  

---

# ðŸ“… Day 5 â€“ Pandas GroupBy, Aggregate & Pivot Table

## âœ… Topics Covered
- **GroupBy basics**
  - `df.groupby("col")["target"].mean()`
- **Multiple aggregations**
  - `agg(["mean","max","min"])`
- **GroupBy with multiple columns**
  - `df.groupby(["col1","col2"])["target"].sum()`
- **Pivot tables**
  - `df.pivot_table(index="col", values="target", aggfunc="mean")`

## ðŸŽ¯ Practice Task
1. Ek **sales DataFrame** banao with columns: `Region, Product, Sales, Profit` (8 rows).  
2. Group by `Region` â†’ total **Sales** nikalo.  
3. Group by `Product` â†’ average **Profit** nikalo.  
4. Ek **Pivot Table** banao jisme:
   - Rows = Region  
   - Columns = Product  
   - Values = Sales sum  

---

# ðŸ“… Day 6 â€“ Pandas Merge, Join & Concat

## âœ… Topics Covered
- **Concatenation**
  - `pd.concat([df1, df2])` â†’ vertical (rows) combine
  - `pd.concat([df1, df2], axis=1)` â†’ horizontal (columns) combine
- **Merge (SQL JOIN style)**
  - `pd.merge(df1, df2, on="col", how="inner")`
- **Different Types of Joins**
  - `how="left"` â†’ Left Join
  - `how="right"` â†’ Right Join
  - `how="outer"` â†’ Full Outer Join
- **Join using Index**
  - `df1.join(df2, how="outer")`

## ðŸŽ¯ Practice Task
1. Ek **students DataFrame** banao with columns:  
   `RollNo, Name, Class`.  
2. Ek **marks DataFrame** banao with columns:  
   `RollNo, Subject, Marks`.  
3. Merge karke ek final DataFrame banao jisme:  
   `Student Name + Subject + Marks` ho.  
4. Concatenate karke do alag batches ka students data ek hi DataFrame me lao.  

---

# ðŸ“… Day 7 â€“ Pandas GroupBy & Aggregation

## âœ… Topics Covered
- **Basic GroupBy**
  - `df.groupby("col")["val"].mean()` â†’ group-wise average
- **Multiple Aggregations**
  - `agg(["mean","sum","max","min"])` â†’ ek group ke multiple stats
- **GroupBy on Multiple Columns**
  - `df.groupby(["col1","col2"])["val"].mean()`
- **Sorting Aggregated Results**
  - `.sort_values(ascending=False)`

## ðŸŽ¯ Practice Task
1. Ek **sales DataFrame** banao jisme columns ho:  
   `Region, Product, Sales`.  
2. `groupby()` karke har **Region ka total sales** nikaalo.  
3. Har **Product ka average sales** nikaalo.  
4. Top 1 product identify karo jo sabse zyada bikta hai.  

---

# ðŸ“… Day 8 â€“ Pandas Data Cleaning & Preprocessing

## âœ… Topics Covered
- **Missing Values**
  - `df.isnull()` â†’ missing check
  - `df.dropna()` â†’ drop missing rows
  - `df.fillna(value)` â†’ fill missing values
- **Duplicates**
  - `df.duplicated()` â†’ duplicate check
  - `df.drop_duplicates()` â†’ remove duplicates
- **Rename + Replace**
  - `df.rename(columns={"old":"new"})`
  - `df["col"].replace(old, new)`
- **String Cleaning**
  - `.str.strip()` â†’ trim spaces  
  - `.str.upper()` â†’ uppercase  
  - `.str.title()` â†’ title case  

## ðŸš€ Practice Task
1. Ek DataFrame banao jisme:
   - Kuch `NaN` values
   - Duplicate rows
   - Extra spaces in strings  
2. Data cleaning steps apply karke ek **final clean dataset** banao.  
3. GitHub pe upload karo: **pandas_cleaning_day8.ipynb**  

---

## âœ… Expected Output
Tumhe real-world **data cleaning pipeline** samajh aayega:
- Missing values handle karna
- Duplicates remove karna
- Columns rename/replace
- String strip/title/upper  

âš¡ Ye freelancing aur industry projects me sabse zyada use hoti skill hai.  

---

# ðŸ“… Day 9 â€“ Exploratory Data Analysis (EDA â€“ Part 1)

## âœ… Topics Covered
- **Dataset Load & Info**
  - `pd.read_csv()` â†’ dataset import
  - `.head()`, `.info()`, `.describe()` â†’ dataset samajhna
- **Univariate Analysis**
  - `.mean()`, `.value_counts()`, `.unique()` â†’ single column analysis
- **Bivariate Analysis**
  - `.groupby()` â†’ relation between two columns
- **Visualization (Matplotlib Basics)**
  - Histogram â†’ distribution
  - Bar Plot â†’ average comparison
  - Scatter Plot â†’ relation between numeric columns

## ðŸš€ Practice Task
Dataset: **tips.csv**  
1. Find **highest tip kis day pe di gayi**  
2. Calculate **Male vs Female average bill difference**  
3. Scatter plot banao: `size` vs `total_bill`  

Upload: **eda_day9.ipynb** to GitHub.

---

## âœ… Expected Output
- Dataset ka **overview** samajh aayega  
- Univariate + Bivariate analysis clear hoga  
- Basic **EDA plots** (histogram, bar, scatter) seekh jaoge  

âš¡ Ye har Kaggle project aur freelancing analysis ka **Step-1** hai.  

---

# ðŸ“… Day 10 â€“ Exploratory Data Analysis (EDA â€“ Part 2, Seaborn)

## âœ… Topics Covered
- **Seaborn Setup**
  - `sns.histplot`, `sns.boxplot` â†’ distribution + outliers
  - `sns.barplot`, `sns.countplot` â†’ categorical comparison
  - `sns.scatterplot`, `sns.lineplot` â†’ relationships
  - `sns.heatmap` â†’ correlation matrix

## ðŸ“Š Visualizations
1. **Distribution Plots**
   - Histogram (total_bill)
   - Boxplot (bill per day)
2. **Categorical Plots**
   - Barplot (avg bill by day)
   - Countplot (gender count)
3. **Relationship Plots**
   - Scatterplot (bill vs tip, hue=sex)
   - Lineplot (bill by group size)
4. **Heatmap**
   - Correlation matrix

## ðŸš€ Practice Task
Dataset: **tips.csv**  
1. Male vs Female **average tip** ka barplot banao.  
2. Day-wise **bill distribution** ka boxplot banao.  
3. Heatmap banao aur identify karo â†’ **tip ke saath sabse zyada correlated column** kaunsa hai.  

---

## âœ… Expected Output
- Distribution, categorical, relationship aur heatmap plots ka mastery.  
- Gender, day aur correlation-based analysis kar paoge.  
- EDA ke visualization step me confidence aayega ðŸš€  


# ðŸ“… Day 11 â€“ Mini Project: Sales Analysis

## âœ… Topics Covered
- Dataset Creation / Import (Superstore-style sales data)
- Data Cleaning:
  - Missing values check
  - Duplicate handling
  - String/category cleanup
- Exploratory Data Analysis:
  - Revenue calculation
  - Top-selling product
  - Region-wise sales
- Visualization (Seaborn + Matplotlib):
  - Product-wise sales (bar plot)
  - Region-wise sales (bar plot)
  - Correlation heatmap

---

## ðŸš€ Practice Task
1. Har **Region ka total revenue** nikalo.  
2. Identify karo **sabse zyada revenue generate karne wala product**.  
3. Ek **Bar Chart** banao â†’ Product vs Revenue.  
4. Ek **Pie Chart** banao â†’ Region-wise sales percentage.  

---

## âœ… Expected Output
- Tum ek **Mini Sales Analysis Project** complete kar loge.  
- Portfolio ke liye ek **freelancing-style analysis notebook** ready hoga.  
- Real-world workflow (Data Cleaning â†’ EDA â†’ Visualization â†’ Insights) clear ho jayega ðŸš€  


# ðŸ“… Day 12 â€“ Web Scraping with Python (Part 1)

## âœ… Topics Covered
- **Libraries for Scraping**
  - `requests` â†’ website ka HTML fetch karna
  - `BeautifulSoup` â†’ HTML parse & extract
  - `pandas` â†’ scraped data ko CSV me save karna
- **Basic Steps**
  - Send request â†’ `requests.get(url)`
  - Parse HTML â†’ `BeautifulSoup(html, "html.parser")`
  - Extract elements â†’ `find_all("tag")`
  - Save to CSV with pandas

---

## ðŸš€ Practice Task
1. Website: **books.toscrape.com**  
   - Scrape book **title**, **price**, **rating**  
2. Data ko **DataFrame** me save karo  
3. Export CSV: **books_prices.csv**  
4. Upload on GitHub: **web_scraping_day12.ipynb**

---

## âœ… Expected Output
- Tum ek **basic web scraper** bana paoge.  
- Titles, prices, ratings jaisa **structured dataset** extract kar loge.  
- Freelancing ke liye ekdum useful skill (clients aksar data scraping mangte hain).  
- Next step: Pagination & multiple pages scrape karna (Day-13). ðŸš€  


# ðŸ“… Day 13 â€“ Web Scraping with Python (Part 2 / Advanced)

## âœ… Topics Covered

* **Advanced Scraping Concepts**

  * Pagination â†’ multiple pages scrape karna
  * Looping through URLs â†’ automate scraping
  * Extract multiple fields â†’ title, price, rating
  * Store results in **lists** â†’ then convert to DataFrame
* **Saving Data**

  * `pandas.DataFrame()` â†’ structured data
  * `to_csv()` â†’ export multiple-page data

---

## ðŸš€ Practice Task

1. Website: **books.toscrape.com**

   * Scrape **first 10 pages**
   * Extract: **title**, **price**, **rating**
2. Store all data in a **single DataFrame**
3. Export CSV: **books\_full.csv**
4. Upload on GitHub: **web\_scraping\_day13.ipynb**

---

## âœ… Expected Output

* Tum ek **multi-page web scraper** bana paoge
* Titles, prices, ratings jaisa **complete dataset** extract kar loge
* Freelancing ke liye **ready-to-use dataset** milega
* Next step: Scraped data ko **Pandas / Excel me clean & analyze** karna (Day-14) ðŸš€


# ðŸ“… Day 14 â€“ Scraped Data Cleaning & Analysis

## âœ… Topics Covered

* **Data Loading**

  * Import scraped CSV into pandas DataFrame
* **Data Cleaning**

  * Remove currency symbols & convert Price â†’ numeric
  * Convert Rating (Oneâ€“Five) â†’ numeric scale (1â€“5)
  * Handle missing values / duplicates
* **Data Analysis**

  * Average price, highest priced book
  * Average rating
  * Grouping & aggregation
* **Visualization**

  * Price distribution (histogram)
  * Ratings count (bar chart)

---

## ðŸš€ Practice Task

1. Extract **Top 10 most expensive books**
2. Calculate **average price per rating** (1â€“5)
3. Create **bar chart** â†’ Rating vs Average Price
4. Export cleaned dataset â†’ **books\_cleaned.csv**
5. Upload notebook â†’ **web\_scraping\_day14.ipynb**

---

## âœ… Expected Output

* Tum ek **scraped dataset ko clean & analyze** kar paoge
* Exported file: **books\_cleaned.csv** (ready for freelancing use)
* Clear **visual insights** (price trends, rating distribution)
* Next step (Day-15): **Automated Dashboard / Report (Excel / PowerBI)** ðŸš€

# ðŸ“… Day 15 â€“ Automated Dashboard / Report

## âœ… Topics Covered

* **Load Cleaned Dataset**

  * Import `books_cleaned.csv` into pandas DataFrame
* **Summary Metrics**

  * Total Books, Average Price, Average Rating
* **Visualizations**

  * Price Distribution (histogram)
  * Top 10 Expensive Books (bar chart)
  * Rating vs Average Price (bar chart)
* **Automated Excel Report**

  * Multiple sheets: All Books, Top 10 Expensive, Avg Price by Rating
  * Ready-to-share client deliverable

---

## ðŸš€ Practice Task

1. Display summary metrics: **Total Books, Average Price, Average Rating**
2. Plot: Price Distribution, Top 10 Expensive Books, Rating vs Average Price
3. Export **Excel report** with multiple sheets: `books_report.xlsx`
4. Upload notebook â†’ **web\_scraping\_day15\_dashboard.ipynb**

---

## âœ… Expected Output

* Tum ek **automated dashboard / report** create kar paoge
* Cleaned dataset + plots + summary metrics **ek hi file / Excel report me ready**
* Freelancing ke liye **professional deliverable**
* Next step (Day-16): **Portfolio polishing + GitHub README / showcase ready** ðŸš€


# ðŸ“… Day 16 â€“ Portfolio & GitHub Showcase

## âœ… Topics Covered

* **Git & GitHub Basics**

  * Create repo â†’ `git init`
  * Track changes â†’ `git add .` + `git commit -m "message"`
  * Push to GitHub â†’ `git push origin main`

* **README.md Polish**

  * Har project ke liye description likho
  * Structure: Problem â†’ Approach â†’ Solution â†’ Output (screenshots / CSV / plots)

* **Portfolio Repo Structure**

  * Projects folder ke andar:

    * `Project_01_SalesAnalysis/`
    * `Project_02_WebScraping/`
    * `Project_03_Dashboard/`
  * Har project folder me ek `README.md` + code/data

* **GitHub Profile Setup**

  * Profile README banayo (intro + skills + contact)
  * Projects ko **pin** karo (Sales Analysis, Web Scraping, Dashboard)

---

## ðŸš€ Practice Task

1. GitHub profile me ek **professional repo structure** banao.
2. Har project ko **alag repo** me push karo.
3. Har repo ke liye ek **portfolio-style README.md** likho.
4. Profile README banao jisme tumhari **skills + projects + contact links** ho.

---

## âœ… Expected Output

* Ek **polished GitHub profile** jo tumhara portfolio show kare.
* Recruiters/clients ko lage ki tumne **real projects** kiye hain.
* Freelancing aur internship ke liye tumhari profile **ready-to-share** hogi. ðŸš€


# ðŸ“… Day 17 â€“ SQL Practice with SSMS (Sales Database)

## âœ… Topics Covered

* **Database Creation**
  * SSMS me ek naya Database banao â†’ `SalesDB`
* **Table Creation**
  * Sales table with columns: id, product, category, price, quantity
* **Data Insertion**
  * Insert dummy product records (10â€“15 rows minimum)
* **Queries**
  * View all data (`SELECT *`)
  * Filtered data (`WHERE price > ...`)
  * Aggregation (`GROUP BY category â†’ total revenue`)
* **Screenshots for Notes**
  * Queries + Outputs capture karke Day-17 notes.md me daalo

---

## ðŸš€ Practice Task

1. Create **SalesDB** database in SSMS  
2. Insert at least **10â€“15 sample product records**  
3. Run queries:
   * View all data
   * Filter products by price
   * Category-wise revenue calculation  
4. Take **screenshots of results** and add them in `Day-17/notes.md`  
5. Upload to GitHub â†’ portfolio roadmap continue karo ðŸš€

---

## âœ… Expected Output

* Tum ek **Sales Database** SSMS me successfully bana loge  
* Basic SQL commands practice karoge: **CREATE, INSERT, SELECT, WHERE, GROUP BY**  
* Screenshots ke saath tumhare GitHub roadmap me ek **solid SQL practice proof** add ho jayega  
* Freelancing / interview ke liye tumhe **SQL confidence boost** milega âœ…


# ðŸ“… Day 18 â€“ SQL Intermediate (Joins & Aggregations)

## âœ… Topics Covered

* **Customers Table Creation**
  * Ek alag table banakar customer details store karna  
* **Linking Tables**
  * `ALTER TABLE` se Sales aur Customers ko connect karna (via `customer_id`)  
* **Joins**
  * **INNER JOIN** â†’ Sales + Customers ka matching data
  * **LEFT JOIN** â†’ Sabhi customers ke sath unke sales records
* **Advanced Queries**
  * City-wise revenue calculation
  * Top 3 highest priced products

---

## ðŸ“‚ Database Schema

**sales**  
- id  
- product  
- category  
- price  
- quantity  
- customer_id  

**customers**  
- customer_id  
- name  
- city  

---

## ðŸš€ Practice Task

1. `customers` table banao aur dummy records insert karo  
2. `sales` table ko update karo â†’ `customer_id` ke sath link karo  
3. Queries run karo:
   * INNER JOIN â†’ Sales + Customers ka combined data
   * City-wise revenue calculation  
   * Top 3 highest priced products  
4. Queries aur unke **screenshots** Day-18 `notes.md` me add karo  
5. Upload karo GitHub pe:  
   - `day18_sql_practice.sql` â†’ queries  
   - `notes.md` â†’ explanation + screenshots  
   - `README.md` â†’ portfolio summary  

---

## âœ… Expected Output

* Tum **Joins (INNER + LEFT)** ka use samjhoge  
* City-wise & customer-wise revenue calculate kar paoge  
* Advanced filtering aur top-N queries likhna seekh jaoge  
* Freelancing clients ke liye **customer + sales linked reports** generate karna easy ho jayega  
* Portfolio me ek aur **professional SQL project** add ho jayega ðŸš€


# ðŸ“… Day 19 â€“ SQL Advanced Filtering & Functions

## âœ… Topics Covered

* **Advanced Filtering**
  * `WHERE` clause with conditions
  * `BETWEEN`, `IN`, `LIKE`, `IS NULL`
* **ORDER BY**
  * Multiple columns ke sath sorting (ASC / DESC)
* **SQL Functions**
  * **String Functions** â†’ `LEN()`, `UPPER()`, `LOWER()`, `SUBSTRING()`
  * **Date Functions** â†’ `GETDATE()`, `YEAR()`, `MONTH()`
  * **Numeric Functions** â†’ `ROUND()`, `ABS()`

---

## ðŸš€ Practice Task

1. **Filtering Queries**
   * Price **1000 aur 50000 ke beech**
   * Product only `Laptop` ya `Phone`
   * Customers jinka naam `N` se start hota ho
   * Customers jinki **sales NULL** hai  

2. **Functions Practice**
   * Customer name length à¤¨à¤¿à¤•à¤¾à¤²à¤¨à¤¾
   * Product ko `UPPERCASE` me dikhana
   * Price ko round off karna (nearest hundred)
   * Current date + Year extract karna  

3. **ORDER BY**
   * City wise ascending + Name wise descending sort  

4. Queries run karke screenshots `notes.md` me daalo  
5. Upload GitHub pe:
   - `day19_sql_practice.sql` â†’ queries
   - `notes.md` â†’ explanation + screenshots
   - `README.md` â†’ portfolio roadmap summary  

---

## âœ… Expected Output

* Tum **filtering** ka mastery kar loge  
* **String, Date, Numeric functions** ka real-world use samjhoge  
* Data ko **clean & formatted** way me dikhana seekh jaoge  
* Freelancing ke liye client ke reports aur formatted SQL outputs banane me expert ban jaoge ðŸš€


# ðŸ“… Day 20 â€“ SQL Aggregations & Subqueries

## âœ… Topics Covered

* **Aggregate Functions**
  * `SUM()`, `AVG()`, `COUNT()`, `MAX()`, `MIN()`
* **GROUP BY**
  * Category / City wise totals
* **HAVING**
  * Aggregated groups pe condition lagana
* **Subqueries**
  * Ek query ke andar dusri query (nested queries)

---

## ðŸš€ Practice Task

1. **Aggregate Functions**
   * Total revenue (`SUM`)
   * Average product price (`AVG`)
   * Number of customers (`COUNT`)
   * Max & Min product price (`MAX`, `MIN`)

2. **GROUP BY + HAVING**
   * City-wise total revenue calculate karo
   * Sirf wahi cities show karo jinka revenue **50,000 se zyada** ho

3. **Subqueries**
   * Products jo **average price se zyada costly** hain
   * Customers jinhone **Laptop purchase kiya** hai

4. Queries run karke screenshots `notes.md` me daalo  
5. Upload GitHub pe:
   - `day20_sql_practice.sql` â†’ queries
   - `notes.md` â†’ explanation + screenshots
   - `README.md` â†’ portfolio roadmap summary  

---

## âœ… Expected Output

* Tum **aggregation reporting** kar paoge  
* **City/Product wise summaries** easily generate kar paoge  
* **Subqueries** ki help se complex problems solve karna aa jayega  
* Freelancing aur interviews ke liye **professional SQL reporting skills** develop ho jayegi ðŸš€


# ðŸ“… Day 21 â€“ SQL Views, Indexes & Stored Procedures

## âœ… Topics Covered
* **Views** â†’ Reusable queries banane ke liye  
* **Indexes** â†’ Query performance fast karne ke liye  
* **Stored Procedures** â†’ Queries ko automate karne ke liye  

---

## ðŸš€ Practice Queries

### ðŸ”¹ Views
* City-wise revenue ka ek **View** banao aur use query me reuse karo

### ðŸ”¹ Indexes
* Customers table ke `City` column pe index add karo  
* Sales table ke `Product` column pe index add karo  

### ðŸ”¹ Stored Procedures
* Ek stored procedure banao jo **Top N products by revenue** return kare  
* Procedure ko execute karke output verify karo  

---

## ðŸ“‚ Files
* `day21_queries.sql` â†’ saare SQL queries  
* `outputs/` â†’ screenshots ya exported CSVs  

---

## âœ… Expected Output
* Tum ek **View** bana paoge jo multiple reports me reuse hoga  
* **Indexes** ki wajah se queries fast execute hongi  
* **Stored Procedure** se tum automated reports generate kar paoge  

---

## ðŸŒŸ Portfolio Value
âœ” Adds **professional database skills** (freelancing & jobs me high demand)  
âœ” Shows you can **optimize queries** with indexes  
âœ” Proof that you can **automate reporting** with stored procedures ðŸš€  


# ðŸ“… Day 22 â€“ SQL Advanced Joins, Nested Queries & Constraints

## âœ… Topics Covered
* **Self JOIN** â†’ ek hi table ke andar join  
* **FULL OUTER JOIN** â†’ dono tables ka complete data  
* **Nested Subqueries** â†’ query ke andar query  
* **Constraints** â†’ data ko validate karne ke liye  
  - NOT NULL  
  - UNIQUE  
  - CHECK  
  - DEFAULT  

---

## ðŸš€ Practice Tasks

### ðŸ”¹ Self JOIN
* Customers table me **self join** karke same city ke customers find karo  

### ðŸ”¹ FULL OUTER JOIN
* Sales aur Customers ka **full outer join** karke dono tables ka data combine karo  

### ðŸ”¹ Nested Subqueries
* Customers identify karo jinhone **average price se zyada ke products** kharide hain  

### ðŸ”¹ Constraints
* Ek `Employees` table banao with constraints:  
  - `NOT NULL` â†’ Name  
  - `UNIQUE` â†’ Email  
  - `CHECK` â†’ Age >= 18  
  - `DEFAULT` â†’ City = 'Unknown'  

---

## ðŸ“‚ Files
* `day22_queries.sql` â†’ saare SQL queries  
* `outputs/` â†’ screenshots ya exported CSVs  

---

## âœ… Expected Output
* Tum **advanced joins** use karke complex data analysis kar paoge  
* **Nested queries** se complex reports generate kar paoge  
* **Constraints** se tum data integrity maintain karna seekh jaoge  

---

## ðŸŒŸ Portfolio Value
âœ” Proof ki tum **complex joins** aur **subqueries** handle kar sakte ho  
âœ” Freelancing ke liye **data validation + integrity rules** apply karna aata hai  
âœ” Adds strong SQL concepts (industry me high demand) ðŸš€  


# ðŸ“… Day 23 â€“ SQL Aggregations & Window Functions

## ðŸ“Œ Overview
Day-23 me maine **Advanced SQL Reporting & Analytics** cover kiya jisme aggregation aur window functions use karke professional level queries banayi.  
Ye skills freelancing, data analytics, aur real-world reporting ke liye directly useful hain.  

---

## âœ… Topics Mastered
- **Aggregate Functions** â†’ SUM(), AVG(), COUNT(), MIN(), MAX()  
- **GROUP BY** â†’ category / city wise data grouping  
- **HAVING** â†’ aggregated data par conditions lagana  
- **Window Functions**  
  - ROW_NUMBER() â†’ row-wise numbering  
  - RANK() â†’ ranking based on sales  
  - PARTITION BY â†’ customer/product wise segmentation  
  - AVG() OVER() â†’ overall average ke sath comparison  


# ðŸ“… Day 25 â€“ SQL Correlated Subqueries & Conditional Logic

## ðŸ“Œ Overview
Day-25 me maine **advanced SQL querying techniques** seekhe jo **dynamic reports aur business insights** ke liye use hote hain.  
Isme focus tha **Correlated Subqueries** (row-by-row comparison) aur **CASE WHEN** (conditional reporting).  

---

## âœ… Topics Mastered
- **Correlated Subqueries** â†’ Subquery jo har row ke liye outer query ke context me run hoti hai  
- **CASE WHEN (Conditional Logic)** â†’ Queries me IF-ELSE type decision making  
- **Subquery + CASE combo** â†’ Smart reporting & business insights ke liye powerful approach


# ðŸ“… Day 26 â€“ SQL Transactions & ACID Properties

## ðŸ“Œ Overview
Day-26 me maine **transactions aur ACID principles** master kiye, jo **financial systems, ecommerce aur banking applications** ke liye backbone hote hain.  
Transactions ensure karte hain ki multiple operations **ya to saath me complete ho ya cancel ho jaye** â†’ data hamesha safe aur consistent rahta hai.  

---

## âœ… Topics Mastered
- **Transactions (BEGIN, COMMIT, ROLLBACK)** â†’ All-or-Nothing execution  
- **ACID Properties**  
  - Atomicity â†’ Sabhi steps execute ho ya koi nahi  
  - Consistency â†’ Database valid state me rahe  
  - Isolation â†’ Parallel transactions ek dusre ko disturb na kare  
  - Durability â†’ Commit ke baad data permanent  
- **Savepoints** â†’ Transactions ke beech checkpoints  
- **Isolation Levels** â†’ Transactions ke concurrency control  

---

# ðŸ“… Day 27 â€“ SQL Triggers (AFTER & INSTEAD OF)

## ðŸ“Œ Overview
Day-27 me maine **SQL Triggers** master kiye.  
Triggers wo **automatic actions** hote hain jo INSERT, UPDATE, DELETE par run hote hain.  
Isse databases khud apne aap **audit logs maintain**, **data validation**, aur **auto-calculations** kar sakte hain â€“ bina manual code likhe.

---

## âœ… Topics Mastered
- **Trigger Basics** â†’ Auto execution on data changes  
- **AFTER Triggers** â†’ Action hone ke baad run hote hain (e.g. audit logs)  
- **INSTEAD OF Triggers** â†’ Original action replace hota hai custom logic se  
- **Use Cases** â†’  
  - Audit trails maintain karna  
  - Automatic calculations  
  - Data validation & restrictions  

---
